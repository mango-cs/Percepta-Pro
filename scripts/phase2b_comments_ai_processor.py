#!/usr/bin/env python3
"""
Percepta Pro Phase 2B: Advanced Comments AI Processing Pipeline
Enhanced bilingual processing with original vs translated text logic
Focus on accuracy, threat detection, and comprehensive data enhancement
"""

import pandas as pd
import numpy as np
import logging
import re
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
from collections import Counter
import json

# AI Processing Imports
try:
    from googletrans import Translator
    TRANSLATOR_AVAILABLE = True
except ImportError:
    TRANSLATOR_AVAILABLE = False
    print("âš ï¸ googletrans not available - translation features limited")

try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ transformers not available - AI sentiment analysis limited")

try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("âš ï¸ NLTK not available - keyword extraction limited")

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'scripts/logs/phase2b_comments_ai_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class Phase2BCommentsAIProcessor:
    """
    Advanced Comments AI Processing Pipeline
    Handles bilingual processing with original vs translated text logic
    """
    
    def __init__(self):
        """Initialize Phase 2B Comments AI Processor"""
        logger.info("ğŸš€ Initializing Phase 2B Comments AI Processor")
        
        # Initialize AI components
        self.translator = None
        self.english_sentiment_analyzer = None
        self.telugu_sentiment_analyzer = None
        self.multilingual_sentiment_analyzer = None
        
        # Processing statistics
        self.stats = {
            'comments_processed': 0,
            'translations_improved': 0,
            'sentiment_analyses_en': 0,
            'sentiment_analyses_te': 0,
            'keywords_extracted': 0,
            'threats_detected': 0,
            'errors': [],
            'start_time': datetime.now()
        }
        
        # Crisis detection patterns
        self.threat_patterns = self._initialize_threat_patterns()
        
        # Initialize components
        self._initialize_ai_components()
        
        logger.info("âœ… Phase 2B Comments AI Processor initialized")
    
    def _initialize_ai_components(self):
        """Initialize all AI processing components"""
        try:
            # Initialize translator
            if TRANSLATOR_AVAILABLE:
                self.translator = Translator()
                logger.info("âœ… Google Translator initialized")
            
            # Initialize English sentiment analyzer (advanced)
            if TRANSFORMERS_AVAILABLE:
                try:
                    self.english_sentiment_analyzer = pipeline(
                        "sentiment-analysis",
                        model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                        return_all_scores=True
                    )
                    logger.info("âœ… Advanced English sentiment analyzer initialized")
                except Exception as e:
                    try:
                        self.english_sentiment_analyzer = pipeline("sentiment-analysis")
                        logger.info("âœ… Basic English sentiment analyzer initialized (fallback)")
                    except Exception as e2:
                        logger.warning(f"âŒ English sentiment analyzer failed: {e2}")
                
                # Initialize Telugu/Multilingual sentiment analyzer
                try:
                    self.multilingual_sentiment_analyzer = pipeline(
                        "sentiment-analysis",
                        model="nlptown/bert-base-multilingual-uncased-sentiment",
                        return_all_scores=True
                    )
                    logger.info("âœ… Multilingual sentiment analyzer initialized")
                except Exception as e:
                    logger.warning(f"âŒ Multilingual sentiment analyzer failed: {e}")
            
            # Initialize NLTK components
            if NLTK_AVAILABLE:
                try:
                    import nltk
                    nltk.download('punkt', quiet=True)
                    nltk.download('stopwords', quiet=True)
                    logger.info("âœ… NLTK components initialized")
                except Exception as e:
                    logger.warning(f"âŒ NLTK initialization failed: {e}")
                    
        except Exception as e:
            logger.error(f"âŒ AI components initialization error: {e}")
    
    def _initialize_threat_patterns(self) -> Dict[str, List[str]]:
        """Initialize comprehensive threat detection patterns"""
        return {
            'death_threats': {
                'telugu': ['à°šà°šà±à°šà°¿à°ªà±‹', 'à°šà°¾à°µà±', 'à°šà°‚à°ªà±‡à°¸à±à°¤à°¾', 'à°•à±Šà°Ÿà±à°Ÿà±‡à°¸à±à°¤à°¾', 'à°¹à°¤à±à°¯'],
                'english': ['die', 'kill', 'death', 'murder', 'eliminate']
            },
            'black_magic': {
                'telugu': ['à°šà±‡à°¤à°¬à°¡à°¿', 'à°šà±‡à°¤à°¬à°¡', 'à°¤à°‚à°¤à±à°°à°‚', 'à°®à°‚à°¤à±à°°à°‚', 'à°šà±‡à°¤à°µà°¿à°¦à±à°¯'],
                'english': ['black magic', 'witchcraft', 'sorcery', 'occult', 'spell']
            },
            'legal_threats': {
                'telugu': ['à°…à°°à±†à°¸à±à°Ÿà±', 'à°•à±‡à°¸à±', 'à°•à±‹à°°à±à°Ÿà±', 'à°œà±ˆà°²à±', 'à°ªà±‹à°²à±€à°¸à±'],
                'english': ['arrest', 'case', 'court', 'jail', 'police', 'lawsuit']
            },
            'violence': {
                'telugu': ['à°•à±Šà°Ÿà±à°Ÿà±', 'à°¦à±†à°¬à±à°¬', 'à°šà±†à°‚à°ª', 'à°¦à°¾à°¡à°¿', 'à°¹à°¿à°‚à°¸'],
                'english': ['beat', 'hit', 'attack', 'violence', 'assault']
            },
            'reputation_attacks': {
                'telugu': ['à°®à°¾à°—à°‚à°Ÿà°¿', 'à°¦à±Šà°‚à°—', 'à°®à±‹à°¸à°‚', 'à°…à°µà°¿à°¨à±€à°¤à°¿', 'à°­à±à°°à°·à±à°Ÿà±'],
                'english': ['fraud', 'cheat', 'corrupt', 'scam', 'criminal']
            },
            'business_threats': {
                'telugu': ['à°µà±à°¯à°¾à°ªà°¾à°°à°‚', 'à°¦à±‹à°šà±à°•à±', 'à°²à±‚à°Ÿà±€', 'à°¦à°‚à°¡à°—'],
                'english': ['business', 'loot', 'steal', 'rob', 'exploit']
            }
        }
    
    def improve_translation(self, original_text: str, existing_translation: str = None) -> str:
        """
        Improve translation quality with validation and correction
        """
        if not original_text or not original_text.strip():
            return existing_translation or ""
        
        try:
            if self.translator and original_text:
                # Detect if text is primarily Telugu
                telugu_chars = len(re.findall(r'[\u0C00-\u0C7F]', original_text))
                total_chars = len(original_text.replace(' ', ''))
                
                if telugu_chars > total_chars * 0.3:  # Significant Telugu content
                    new_translation = self.translator.translate(original_text, src='te', dest='en').text
                    
                    # Quality check: prefer longer, more detailed translation
                    if existing_translation:
                        if len(new_translation) > len(existing_translation) * 1.2:
                            self.stats['translations_improved'] += 1
                            return new_translation
                        return existing_translation
                    
                    return new_translation
                else:
                    # Text is primarily English, return as-is
                    return original_text
            
            return existing_translation or original_text
            
        except Exception as e:
            logger.error(f"âŒ Translation improvement failed: {e}")
            return existing_translation or original_text
    
    def analyze_sentiment_bilingual(self, original_text: str, translated_text: str) -> Dict[str, Any]:
        """
        Analyze sentiment for both original Telugu and translated English
        Returns separate sentiment scores for each language
        """
        result = {
            'sentiment_score_te': 0.0,
            'sentiment_label_te': 'Neutral',
            'sentiment_score_en': 0.0,
            'sentiment_label_en': 'Neutral',
            'confidence_te': 0.0,
            'confidence_en': 0.0
        }
        
        # Analyze original Telugu text
        if original_text and original_text.strip():
            te_score, te_label, te_conf = self._analyze_single_sentiment(original_text, 'te')
            result['sentiment_score_te'] = te_score
            result['sentiment_label_te'] = te_label
            result['confidence_te'] = te_conf
            self.stats['sentiment_analyses_te'] += 1
        
        # Analyze translated English text
        if translated_text and translated_text.strip():
            en_score, en_label, en_conf = self._analyze_single_sentiment(translated_text, 'en')
            result['sentiment_score_en'] = en_score
            result['sentiment_label_en'] = en_label
            result['confidence_en'] = en_conf
            self.stats['sentiment_analyses_en'] += 1
        
        return result
    
    def _analyze_single_sentiment(self, text: str, language: str) -> Tuple[float, str, float]:
        """
        Analyze sentiment for single text in specified language
        Returns: (score, label, confidence)
        """
        if not text or not text.strip():
            return 0.0, 'Neutral', 0.0
        
        # Limit text length for processing
        if len(text) > 512:
            text = text[:512] + "..."
        
        try:
            # Choose appropriate analyzer
            if language == 'en' and self.english_sentiment_analyzer:
                analyzer = self.english_sentiment_analyzer
            elif language == 'te' and self.multilingual_sentiment_analyzer:
                analyzer = self.multilingual_sentiment_analyzer
            else:
                # Fallback to basic sentiment analysis
                return self._basic_sentiment_analysis(text, language)
            
            results = analyzer(text)
            
            # Handle different result formats
            if isinstance(results[0], list):
                results = results[0]
            
            # Find best result
            best_result = max(results, key=lambda x: x['score'])
            label = best_result['label'].upper()
            confidence = best_result['score']
            
            # Normalize to standard format
            if label in ['POSITIVE', 'POS', '5 STARS', '4 STARS']:
                return confidence, 'Positive', confidence
            elif label in ['NEGATIVE', 'NEG', '1 STAR', '2 STARS']:
                return -confidence, 'Negative', confidence
            else:
                return 0.0, 'Neutral', confidence
                
        except Exception as e:
            logger.error(f"âŒ Sentiment analysis failed for {language}: {e}")
            return self._basic_sentiment_analysis(text, language)
    
    def _basic_sentiment_analysis(self, text: str, language: str) -> Tuple[float, str, float]:
        """Fallback sentiment analysis using keyword matching"""
        text_lower = text.lower()
        
        # Language-specific keywords
        if language == 'te':
            positive_keywords = ['à°šà°¾à°²à°¾ à°¬à°¾à°—à±à°‚à°¦à°¿', 'à°…à°¦à±à°­à±à°¤à°‚', 'à°®à°‚à°šà°¿à°¦à°¿', 'à°¬à°¾à°—à±à°‚à°¦à°¿', 'à°¸à±‚à°ªà°°à±']
            negative_keywords = ['à°šà±†à°¤à±à°¤', 'à°¦à°¾à°°à±à°£à°‚', 'à°®à±‹à°¸à°‚', 'à°…à°°à±†à°¸à±à°Ÿà±', 'à°•à±‡à°¸à±', 'à°šà±‡à°¤à°¬à°¡à°¿']
        else:
            positive_keywords = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'best']
            negative_keywords = ['bad', 'terrible', 'awful', 'worst', 'hate', 'fraud', 'scam']
        
        pos_count = sum(1 for word in positive_keywords if word in text_lower)
        neg_count = sum(1 for word in negative_keywords if word in text_lower)
        
        if pos_count > neg_count:
            return 0.6, 'Positive', 0.6
        elif neg_count > pos_count:
            return -0.6, 'Negative', 0.6
        else:
            return 0.0, 'Neutral', 0.5
    
    def extract_keywords_bilingual(self, original_text: str, translated_text: str) -> Dict[str, str]:
        """
        Extract keywords from both original and translated text
        """
        result = {
            'keywords_te': '',
            'keywords_en': '',
            'critical_keywords_te': '',
            'critical_keywords_en': ''
        }
        
        # Extract Telugu keywords from original
        if original_text:
            te_keywords = self._extract_keywords(original_text, 'te')
            result['keywords_te'] = ', '.join(te_keywords[:10])
            
            # Extract critical Telugu keywords
            critical_te = self._extract_critical_keywords(original_text, 'te')
            result['critical_keywords_te'] = ', '.join(critical_te)
        
        # Extract English keywords from translated
        if translated_text:
            en_keywords = self._extract_keywords(translated_text, 'en')
            result['keywords_en'] = ', '.join(en_keywords[:10])
            
            # Extract critical English keywords
            critical_en = self._extract_critical_keywords(translated_text, 'en')
            result['critical_keywords_en'] = ', '.join(critical_en)
        
        self.stats['keywords_extracted'] += 1
        return result
    
    def _extract_keywords(self, text: str, language: str, max_keywords: int = 15) -> List[str]:
        """Extract general keywords from text"""
        if not text or not text.strip():
            return []
        
        try:
            # Clean and tokenize text
            cleaned_text = re.sub(r'[^\w\s]', ' ', text.lower())
            words = cleaned_text.split()
            
            # Language-specific stop words
            if language == 'te':
                stop_words = {'à°…à°¨à°¿', 'à°†', 'à°ˆ', 'à°‰à°‚à°¦à°¿', 'à°µà±à°‚à°¦à°¿', 'à°šà±‡à°¸à°¿', 'à°…à°¯à°¿', 'à°—à°¾'}
            else:
                stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
            
            # Filter meaningful words
            meaningful_words = [
                word for word in words 
                if len(word) > 2 and word not in stop_words
            ]
            
            # Count frequency and return top keywords
            word_freq = Counter(meaningful_words)
            keywords = [word for word, count in word_freq.most_common(max_keywords)]
            
            return keywords
            
        except Exception as e:
            logger.error(f"âŒ Keyword extraction failed: {e}")
            return []
    
    def _extract_critical_keywords(self, text: str, language: str) -> List[str]:
        """Extract critical/threat-related keywords"""
        critical_keywords = []
        text_lower = text.lower()
        
        # Check all threat patterns
        for threat_type, patterns in self.threat_patterns.items():
            lang_patterns = patterns.get(language, [])
            for pattern in lang_patterns:
                if pattern in text_lower:
                    critical_keywords.append(pattern)
        
        return list(set(critical_keywords))  # Remove duplicates
    
    def detect_threats(self, original_text: str, translated_text: str, comment_data: Dict) -> Dict[str, Any]:
        """
        Comprehensive threat detection in both languages
        """
        threats = {
            'threat_detected': False,
            'threat_level': 'None',
            'threat_types': [],
            'threat_score': 0.0,
            'critical_patterns': []
        }
        
        # Analyze both texts for threats
        te_threats = self._detect_threats_in_text(original_text, 'te')
        en_threats = self._detect_threats_in_text(translated_text, 'en')
        
        # Combine threat analysis
        all_threats = te_threats + en_threats
        
        if all_threats:
            threats['threat_detected'] = True
            threats['threat_types'] = list(set([t['type'] for t in all_threats]))
            threats['threat_score'] = max([t['severity'] for t in all_threats])
            threats['critical_patterns'] = [t['pattern'] for t in all_threats]
            
            # Determine threat level
            if threats['threat_score'] >= 8.0:
                threats['threat_level'] = 'CRITICAL'
            elif threats['threat_score'] >= 6.0:
                threats['threat_level'] = 'HIGH'
            elif threats['threat_score'] >= 4.0:
                threats['threat_level'] = 'MEDIUM'
            else:
                threats['threat_level'] = 'LOW'
            
            self.stats['threats_detected'] += 1
        
        return threats
    
    def _detect_threats_in_text(self, text: str, language: str) -> List[Dict]:
        """Detect specific threat patterns in text"""
        threats = []
        
        if not text:
            return threats
        
        text_lower = text.lower()
        
        # Check each threat category
        for threat_type, patterns in self.threat_patterns.items():
            lang_patterns = patterns.get(language, [])
            
            for pattern in lang_patterns:
                if pattern in text_lower:
                    # Calculate severity based on threat type
                    severity_map = {
                        'death_threats': 9.0,
                        'black_magic': 8.5,
                        'legal_threats': 7.0,
                        'violence': 8.0,
                        'reputation_attacks': 6.0,
                        'business_threats': 5.0
                    }
                    
                    threats.append({
                        'type': threat_type,
                        'pattern': pattern,
                        'severity': severity_map.get(threat_type, 5.0),
                        'language': language
                    })
        
        return threats
    
    def process_comment_row(self, row: pd.Series) -> Dict[str, Any]:
        """
        Process a single comment through the comprehensive AI pipeline
        """
        try:
            # Extract base data
            original_comment = str(row.get('Comment', ''))
            existing_translation = str(row.get('Comment_EN', ''))
            
            # Step 1: Improve translation
            improved_translation = self.improve_translation(original_comment, existing_translation)
            
            # Step 2: Bilingual sentiment analysis
            sentiment_data = self.analyze_sentiment_bilingual(original_comment, improved_translation)
            
            # Step 3: Bilingual keyword extraction
            keyword_data = self.extract_keywords_bilingual(original_comment, improved_translation)
            
            # Step 4: Threat detection
            threat_data = self.detect_threats(original_comment, improved_translation, row.to_dict())
            
            # Step 5: Enhanced metadata processing
            metadata = self._process_enhanced_metadata(row)
            
            # Combine all processed data
            result = {
                # Basic identifiers
                'VideoID': row.get('VideoID', ''),
                'CommentID': row.get('CommentID', ''),
                'ParentID': row.get('ParentID', ''),
                'IsReply': row.get('IsReply', False),
                'Author': row.get('Author', ''),
                
                # Content (improved)
                'Comment': original_comment,
                'Comment_EN': improved_translation,
                
                # Bilingual sentiment (enhanced)
                'SentimentScore_TE': sentiment_data['sentiment_score_te'],
                'SentimentLabel_TE': sentiment_data['sentiment_label_te'],
                'SentimentScore_EN': sentiment_data['sentiment_score_en'],
                'SentimentLabel_EN': sentiment_data['sentiment_label_en'],
                'SentimentConfidence_TE': sentiment_data['confidence_te'],
                'SentimentConfidence_EN': sentiment_data['confidence_en'],
                
                # Keywords (bilingual)
                'Keywords_TE': keyword_data['keywords_te'],
                'Keywords_EN': keyword_data['keywords_en'],
                'CriticalKeywords_TE': keyword_data['critical_keywords_te'],
                'CriticalKeywords_EN': keyword_data['critical_keywords_en'],
                
                # Threat detection
                'ThreatDetected': threat_data['threat_detected'],
                'ThreatLevel': threat_data['threat_level'],
                'ThreatTypes': ', '.join(threat_data['threat_types']),
                'ThreatScore': threat_data['threat_score'],
                'CriticalPatterns': ', '.join(threat_data['critical_patterns']),
                
                # Enhanced metadata
                **metadata
            }
            
            self.stats['comments_processed'] += 1
            
            if self.stats['comments_processed'] % 100 == 0:
                logger.info(f"âœ… Processed {self.stats['comments_processed']} comments")
            
            return result
            
        except Exception as e:
            error_msg = f"âŒ Comment processing failed: {e}"
            logger.error(error_msg)
            self.stats['errors'].append(error_msg)
            return self._create_error_result(row)
    
    def _process_enhanced_metadata(self, row: pd.Series) -> Dict[str, Any]:
        """Process enhanced metadata with proper formatting"""
        return {
            'LikeCount': int(row.get('LikeCount', 0)) if pd.notna(row.get('LikeCount')) else 0,
            'ReplyCount': 0,  # Will be calculated later
            'Date': row.get('Date', ''),
            'Date_Formatted': row.get('Date_Formatted', ''),
            'PublishedAt': row.get('PublishedAt', ''),
            'UpdatedAt': row.get('UpdatedAt', ''),
            'ModerationStatus': row.get('ModerationStatus', 'published'),
            'ProcessingStatus': 'completed',
            'ProcessingTimestamp': datetime.now().isoformat(),
            'AIEnhanced': True
        }
    
    def _create_error_result(self, row: pd.Series) -> Dict[str, Any]:
        """Create error result for failed processing"""
        return {
            'VideoID': row.get('VideoID', ''),
            'CommentID': row.get('CommentID', ''),
            'Comment': str(row.get('Comment', '')),
            'Comment_EN': str(row.get('Comment_EN', '')),
            'ProcessingStatus': 'error',
            'ProcessingTimestamp': datetime.now().isoformat(),
            'AIEnhanced': False
        }
    
    def process_comments_dataset(self, input_file: str, output_file: str):
        """
        Process entire comments dataset through Phase 2B AI pipeline
        """
        logger.info("ğŸš€ Starting Phase 2B Comments AI Processing")
        logger.info("=" * 70)
        
        # Load dataset
        logger.info(f"ğŸ“‚ Loading comments dataset: {input_file}")
        df = pd.read_csv(input_file)
        logger.info(f"ğŸ“Š Loaded {len(df)} comments")
        
        # Process comments
        processed_comments = []
        
        for idx, row in df.iterrows():
            processed_comment = self.process_comment_row(row)
            processed_comments.append(processed_comment)
            
            # Progress reporting
            if (idx + 1) % 50 == 0:
                progress = ((idx + 1) / len(df)) * 100
                logger.info(f"ğŸ”„ Processing progress: {progress:.1f}% ({idx + 1}/{len(df)})")
        
        # Create enhanced DataFrame
        enhanced_df = pd.DataFrame(processed_comments)
        
        # Calculate reply counts
        enhanced_df = self._calculate_reply_counts(enhanced_df)
        
        # Save enhanced dataset
        enhanced_df.to_csv(output_file, index=False)
        
        # Generate comprehensive report
        self._generate_processing_report(enhanced_df, output_file)
        
        logger.info(f"âœ… Phase 2B Comments AI Processing completed!")
        logger.info(f"ğŸ“ Enhanced dataset saved: {output_file}")
        
        return enhanced_df
    
    def _calculate_reply_counts(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calculate reply counts for comments"""
        reply_counts = df.groupby('ParentID').size().to_dict()
        
        df['ReplyCount'] = df['CommentID'].map(reply_counts).fillna(0).astype(int)
        
        return df
    
    def _generate_processing_report(self, df: pd.DataFrame, output_file: str):
        """Generate comprehensive processing report"""
        processing_time = datetime.now() - self.stats['start_time']
        
        report = {
            'phase': 'Phase 2B - Comments AI Enhancement',
            'timestamp': datetime.now().isoformat(),
            'processing_time': str(processing_time),
            'dataset_info': {
                'total_comments': len(df),
                'ai_enhanced_comments': (df['AIEnhanced'] == True).sum(),
                'processing_success_rate': f"{((df['ProcessingStatus'] == 'completed').sum() / len(df)) * 100:.1f}%"
            },
            'bilingual_analysis': {
                'telugu_sentiments_analyzed': self.stats['sentiment_analyses_te'],
                'english_sentiments_analyzed': self.stats['sentiment_analyses_en'],
                'translations_improved': self.stats['translations_improved'],
                'keywords_extracted': self.stats['keywords_extracted']
            },
            'threat_detection': {
                'total_threats_detected': self.stats['threats_detected'],
                'critical_threats': (df['ThreatLevel'] == 'CRITICAL').sum(),
                'high_threats': (df['ThreatLevel'] == 'HIGH').sum(),
                'medium_threats': (df['ThreatLevel'] == 'MEDIUM').sum(),
                'threat_detection_rate': f"{(self.stats['threats_detected'] / len(df)) * 100:.1f}%"
            },
            'sentiment_distribution': {
                'telugu_positive': (df['SentimentLabel_TE'] == 'Positive').sum(),
                'telugu_negative': (df['SentimentLabel_TE'] == 'Negative').sum(),
                'telugu_neutral': (df['SentimentLabel_TE'] == 'Neutral').sum(),
                'english_positive': (df['SentimentLabel_EN'] == 'Positive').sum(),
                'english_negative': (df['SentimentLabel_EN'] == 'Negative').sum(),
                'english_neutral': (df['SentimentLabel_EN'] == 'Neutral').sum()
            },
            'data_quality': {
                'avg_telugu_sentiment_score': df['SentimentScore_TE'].mean(),
                'avg_english_sentiment_score': df['SentimentScore_EN'].mean(),
                'comments_with_keywords': (df['Keywords_EN'].str.len() > 0).sum(),
                'comments_with_critical_keywords': (df['CriticalKeywords_EN'].str.len() > 0).sum()
            },
            'errors': {
                'total_errors': len(self.stats['errors']),
                'error_details': self.stats['errors'][:10]  # First 10 errors
            }
        }
        
        # Save report
        report_file = output_file.replace('.csv', '_processing_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # Print summary
        logger.info("=" * 70)
        logger.info("ğŸ“Š PHASE 2B PROCESSING SUMMARY")
        logger.info("=" * 70)
        logger.info(f"âœ… Total Comments Processed: {len(df)}")
        logger.info(f"ğŸ”„ AI Enhancement Success: {report['dataset_info']['processing_success_rate']}")
        logger.info(f"ğŸŒ Translations Improved: {self.stats['translations_improved']}")
        logger.info(f"ğŸ” Keywords Extracted: {self.stats['keywords_extracted']}")
        logger.info(f"âš ï¸ Threats Detected: {self.stats['threats_detected']}")
        logger.info(f"ğŸ“ˆ Processing Time: {processing_time}")
        logger.info(f"ğŸ“ Report Saved: {report_file}")
        logger.info("=" * 70)

def main():
    """Main function to run Phase 2B Comments AI Processing"""
    processor = Phase2BCommentsAIProcessor()
    
    # Input and output files
    input_file = "backend/data/comments/youtube_comments_final.csv"
    output_file = "backend/data/comments/youtube_comments_ai_enhanced.csv"
    
    # Process dataset
    enhanced_df = processor.process_comments_dataset(input_file, output_file)
    
    return enhanced_df

if __name__ == "__main__":
    main() 